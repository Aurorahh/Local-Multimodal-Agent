from transformers import AutoProcessor, AutoModelForCausalLM
from PIL import Image
from .config import Config
import torch
import traceback # å¼•å…¥è¯¦ç»†æŠ¥é”™å·¥å…·

class VisionExpert:
    def __init__(self):
        print(f"ğŸ‘ï¸ æ­£åœ¨åŠ è½½ Florence-2 è§†è§‰ä¸“å®¶: {Config.VISION_MODEL_PATH} ...")
        self.device = "cuda" if torch.cuda.is_available() else "cpu"
        self.torch_dtype = torch.float16 if torch.cuda.is_available() else torch.float32

        try:
            self.model = AutoModelForCausalLM.from_pretrained(
                Config.VISION_MODEL_PATH, 
                torch_dtype=self.torch_dtype, 
                trust_remote_code=True,
                attn_implementation="eager"
            ).to(self.device)
            
            self.processor = AutoProcessor.from_pretrained(
                Config.VISION_MODEL_PATH, 
                trust_remote_code=True
            )
            print("âœ… è§†è§‰ä¸“å®¶åŠ è½½å®Œæ¯•ï¼")
        except Exception as e:
            print(f"âŒ åŠ è½½å¤±è´¥: {e}")
            self.model = None

    def analyze_image(self, image_path, prompt_type="<MORE_DETAILED_CAPTION>", user_question=None):
        if not self.model:
            return "âŒ æ¨¡å‹æœªåŠ è½½"
            
        try:
            # 1. è¯­è¨€æ£€æŸ¥
            if user_question:
                for char in user_question:
                    if '\u4e00' <= char <= '\u9fff':
                        return "âš ï¸ Florence-2 ä»…æ”¯æŒè‹±æ–‡æé—® (Only English supported)."

            print(f"ğŸ” Debug: æ­£åœ¨æ‰“å¼€å›¾ç‰‡ {image_path}")
            image = Image.open(image_path)
            if image.mode != "RGB":
                image = image.convert("RGB")

            # 2. æ„å»º Prompt
            # å…³é”®ï¼š<VQA> åé¢å¿…é¡»æœ‰ç©ºæ ¼ï¼Œé˜²æ­¢ä¸é—®é¢˜ç²˜è¿
            if user_question:
                task_prompt = "<VQA>"
                text_input = task_prompt + " " + user_question 
            else:
                task_prompt = prompt_type
                text_input = task_prompt

            # 3. å¤„ç†è¾“å…¥
            inputs = self.processor(text=text_input, images=image, return_tensors="pt")
            inputs = inputs.to(self.device, self.torch_dtype)

            # 4. ç”Ÿæˆ (å¼ºåˆ¶ Greedy Search)
            generated_ids = self.model.generate(
                input_ids=inputs["input_ids"],
                pixel_values=inputs["pixel_values"],
                max_new_tokens=1024,
                num_beams=1,
                do_sample=False,
                use_cache=False 
            )

            # 5. ã€ç‰©ç†åˆ‡ç‰‡ä¿®å¤ã€‘åªä¿ç•™æ–°ç”Ÿæˆçš„ tokenï¼Œå½»åº•å»é™¤ Prompt å›æ˜¾
            # è·å–è¾“å…¥éƒ¨åˆ†çš„é•¿åº¦
            input_token_len = inputs["input_ids"].shape[1]
            # åªå–è¾“å…¥é•¿åº¦ä¹‹åçš„éƒ¨åˆ†ï¼ˆå³çº¯ç²¹çš„å›ç­”ï¼‰
            new_tokens = generated_ids[0][input_token_len:]
            
            # 6. è§£ç 
            answer = self.processor.decode(new_tokens, skip_special_tokens=True).strip()

            # 7. å…œåº•æ£€æŸ¥ï¼šå¦‚æœæ¨¡å‹è¿˜æ˜¯å‘ç–¯è¾“å‡ºäº† <loc> æ ‡ç­¾
            if "<loc" in answer or answer == "":
                # å°è¯•ç”¨å®˜æ–¹åå¤„ç†å†æ•‘ä¸€æ¬¡
                full_text = self.processor.batch_decode(generated_ids, skip_special_tokens=False)[0]
                parsed = self.processor.post_process_generation(
                    full_text, 
                    task=task_prompt, 
                    image_size=(image.width, image.height)
                )
                return parsed.get(task_prompt, answer)
            
            return answer
            
        except Exception as e:
            print(f"âŒ åˆ†æé”™è¯¯: {e}")
            traceback.print_exc()
            return f"Error: {e}"